<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MNIST手写数字识别——pytorch</title>
      <link href="2020/12/18/MNIST/"/>
      <url>2020/12/18/MNIST/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>MNIST手写数字识别是pytorch的经典项目之一，适用于新手学习pytorch这一深度学习训练平台以及初步了解计算机视觉的相关训练步骤。</p><p>在新手学习pytorch的时候，最好的教程就是<a href="https://pytorch.org/tutorials">Pytorch official Tutorial</a>。以下的网络搭建主要参考pytorch的官方教程完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在windows显示图像的时候可能会遇到一个报错，需要添加这个语句才可以正常通过，意思是允许重复加载动态链接库</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="string">&quot;TRUE&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>首先引入几个重要的Package。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">dataset_root = <span class="string">&#x27;./data&#x27;</span></span><br><span class="line">batch_size  = <span class="number">64</span></span><br><span class="line">epoch_time = <span class="number">20</span></span><br><span class="line">path = <span class="string">&#x27;./checkpoints/MNIST.pth&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接下来需要对几个重要的全局参数进行设置，<code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &#39;cpu&#39;)</code>用来设置device为CPU还是GPU，由于GPU进行矩阵运算更快因此尽可能将网络搭载在GPU上。<br>之后的几个参数也主要是设置数据集存储位置和<code>batch_size</code>和训练轮数<code>epoch_time</code>以及训练网络参数的存储位置<code>path</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line"></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,),(<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.MNIST(root=dataset_root,train = <span class="literal">True</span>,transform=transform,download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,num_workers=<span class="number">0</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.MNIST(root=dataset_root,train= <span class="literal">False</span>,download=<span class="literal">True</span>,transform=transform)</span><br><span class="line"></span><br><span class="line">testloader = torch.utils.data.DataLoader(testset,batch_size=batch_size,num_workers=<span class="number">0</span>,shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这部分代码主要实现数据集的下载和分装。分别加载到对应的数据加载器，方便后续的网络训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">show image test</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">img</span>):</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(npimg.transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line"><span class="comment"># 注意这里不能够使用*.to(device)将数据放到GPU上，因为是要进行图像展示，处理需要在CPU上进行</span></span><br><span class="line">images,labels = dataiter.next()</span><br><span class="line"></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(labels)</span><br></pre></td></tr></table></figure><p>以上部分代码主要实现部分图像的显示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    design a module </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        <span class="comment"># 1*1*28*28</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">8</span>*<span class="number">8</span>,<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>,<span class="number">10</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span> </span><br><span class="line">        x= self.pool(F.relu(self.conv1(x)))   </span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = x.view(<span class="number">-1</span>,<span class="number">16</span>*<span class="number">8</span>*<span class="number">8</span>)</span><br><span class="line">        x= F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络并将网络部署到GPU上</span></span><br><span class="line">net = Net().to(device)</span><br></pre></td></tr></table></figure><p>上面的代码实现了对于训练网络的构建，主要是使用了两层的卷积网络进行特征的提取，然后将features导入到两层全连接linear网络层进行分类。</p><p>然后利用下面代码可以查看网络的参数数量构成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(params)):</span><br><span class="line">    print(params[i].size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line"><span class="comment"># torch.Size([6, 1, 5, 5])</span></span><br><span class="line"><span class="comment"># torch.Size([6])</span></span><br><span class="line"><span class="comment"># torch.Size([16, 6, 5, 5])</span></span><br><span class="line"><span class="comment"># torch.Size([16])</span></span><br><span class="line"><span class="comment"># torch.Size([120, 1024])</span></span><br><span class="line"><span class="comment"># torch.Size([120])</span></span><br><span class="line"><span class="comment"># torch.Size([10, 120])</span></span><br><span class="line"><span class="comment"># torch.Size([10])</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>网络构建完成之后就需要对优化器进行设置，这里我选择使用交叉熵损失函数以及Adam优化方式。learning-rate设置为0.001。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">criterion= nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(),lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><p>网络和训练损失函数以及optimizer都设置好了，可以开始网络的训练过程了，我们的训练次数一共为20次。训练网络部分的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    train the neural network</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epoch_time):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line">        <span class="comment"># 注意这里要将images和labels的数据放到GPU上。</span></span><br><span class="line">        images,labels = data[<span class="number">0</span>].to(device),data[<span class="number">1</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(labels)</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        predicted = net(images)</span><br><span class="line"></span><br><span class="line">        loss = criterion(predicted,labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">            print(<span class="string">&quot;[%d / %5d] loss: %3f&quot;</span> %(epoch+<span class="number">1</span>,i+<span class="number">1</span>,running_loss/<span class="number">100</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;finished training!&quot;</span>)</span><br><span class="line"></span><br><span class="line">torch.save(net.state_dict(),path)</span><br></pre></td></tr></table></figure><p>训练完成之后将网络参数保存到预先设置好的文件中。</p><p>接下来进行网络的测试，我们是用<code>with torch.no_grad():</code>包裹测试代码，使得torch不跟踪运算过程，避免进一步计算梯度降低运算性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">net = Net().to(device)</span><br><span class="line">net.load_state_dict(torch.load(path))</span><br><span class="line"></span><br><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i ,data <span class="keyword">in</span> enumerate(testloader):</span><br><span class="line">        images ,labels = data[<span class="number">0</span>].to(device),data[<span class="number">1</span>].to(device)</span><br><span class="line"></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        print(outputs,labels)</span><br><span class="line"></span><br><span class="line">        _,predicted = torch.max(outputs,<span class="number">1</span>)</span><br><span class="line">        c = (predicted==labels).squeeze()</span><br><span class="line">        loss = criterion(outputs,labels)</span><br><span class="line">        print(loss)</span><br><span class="line">        <span class="comment"># print(c,predicted,labels)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(c)):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label]+= c[i].item()</span><br><span class="line">            class_total[label]+=<span class="number">1</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">&quot;accuracy of Number:%d : %.2f %%&quot;</span>%(i,<span class="number">100</span>*class_correct[i]/class_total[i]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>训练结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">accuracy of Number:<span class="number">0</span> : <span class="number">99.39</span> %</span><br><span class="line">accuracy of Number:<span class="number">1</span> : <span class="number">99.74</span> %</span><br><span class="line">accuracy of Number:<span class="number">2</span> : <span class="number">99.22</span> %</span><br><span class="line">accuracy of Number:<span class="number">3</span> : <span class="number">99.21</span> %</span><br><span class="line">accuracy of Number:<span class="number">4</span> : <span class="number">99.08</span> %</span><br><span class="line">accuracy of Number:<span class="number">5</span> : <span class="number">99.10</span> %</span><br><span class="line">accuracy of Number:<span class="number">6</span> : <span class="number">98.75</span> %</span><br><span class="line">accuracy of Number:<span class="number">7</span> : <span class="number">99.03</span> %</span><br><span class="line">accuracy of Number:<span class="number">8</span> : <span class="number">98.56</span> %</span><br><span class="line">accuracy of Number:<span class="number">9</span> : <span class="number">98.81</span> %</span><br></pre></td></tr></table></figure><p>可以看到对手写数字的识别性能还是很不错的，基本都能达到98%以上。并且这个网络还可以进一步提高。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> neural network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SGG领域论文整理(持续更新)</title>
      <link href="2020/12/14/SGG-Field-Prof/"/>
      <url>2020/12/14/SGG-Field-Prof/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="SGG-综述"><a href="#SGG-综述" class="headerlink" title="SGG 综述"></a>SGG 综述</h2><hr><ul><li><a href="https://arxiv.org/abs/2005.08045"><strong><em>“Visual Relationship Detection using Scene Graphs: A Survey”</em></strong></a><blockquote><p>arXiv检索2020</p><p><a href="https://scholar.google.com/citations?user=oPzYPK0AAAAJ&hl=en">Aniket Agarwal</a> <br><a href="https://vlgiitr.github.io/author/ayush-mangal/">Ayush Mangal</a> <br><a href="https://www.cs.cmu.edu/~goyal/">Vipul</a></p></blockquote></li></ul><h2 id="SGG-论文"><a href="#SGG-论文" class="headerlink" title="SGG 论文"></a>SGG 论文</h2><hr><ul><li><p><a href="https://arxiv.org/abs/1701.02426"><strong><em>“Scene graph generation by iterative message passing”</em></strong></a></p><blockquote><p>CVPR 2017 </p><p><a href="https://cs.stanford.edu/~danfei/">Danfei Xu</a> <br><a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>  <br><a href="https://chrischoy.github.io/">Christopher B. Choy</a> <br><a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei </a></p><p><a href="https://cs.stanford.edu/">Department of Computer Science, Stanford University </a> <br><a href="http://vision.stanford.edu/">Stanford Computer Vision Lab</a></p></blockquote></li><li><p><a href="https://arxiv.org/abs/1704.03114"><strong><em>“Detecting visual relationships with deep relational networks”</em></strong></a></p><blockquote><p>CVPR 2017 oral paper</p><p><a href="http://daibo.info/">Bo Dai</a>   <br>Yuqi Zhang <br><a href="http://dahua.me/">Dahua Lin</a>  </p><p><a href="https://www.ie.cuhk.edu.hk/people/graduate_students.shtml">Department of Information Engineering, The Chinese University of Hong Kong</a></p></blockquote></li><li><p><a href="https://arxiv.org/abs/1707.09700"><strong><em>“Scene graph generation from objects, phrases and region captions”</em></strong></a></p><blockquote><p> ICCV 2017</p><p><a href="http://cvboy.com/">Yikang Li</a> <br><a href="https://wlouyang.github.io/">Wanli Ouyang</a> <br><a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a> <br>Kun Wang <br><a href="https://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>  </p><p>The Chinese University of Hong Kong, Hong Kong SAR, China</p></blockquote></li><li><p><a href="https://arxiv.org/abs/1806.11538"><strong><em>“Factorizable net: an efficient subgraph-based framework for scene graph generation”</em></strong></a></p><blockquote><p>ECCV 2018 </p><p><a href="http://cvboy.com/">Yikang Li</a> <br><a href="https://wlouyang.github.io/">Wanli Ouyang</a>Wanli Ouyang  <br><a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a>  <br>Jianping Shi <br>Chao Zhang <br><a href="https://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a> </p><p>The Chinese University of Hong Kong, Hong Kong SAR, China</p></blockquote></li><li><p><a href="https://arxiv.org/abs/1711.06640"><strong><em>“Neural motifs: Scene graph parsing with global context”</em></strong></a></p><blockquote><p>CVPR 2018 camera ready</p><p><a href="https://rowanzellers.com/">Rowan Zellers</a> <br><a href="http://markyatskar.com/">Mark Yatskar</a> <br><a href="http://samthomson.com/">Sam Thomson</a> <br><a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a></p><p><a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science &amp; Engineering, University of Washington</a></p></blockquote></li><li><p><a href="https://arxiv.org/abs/1808.00191"><strong><em>“Graph r-cnn for scene graph generation”</em></strong></a></p><blockquote><p>ECCV 2018 camera ready</p><p><a href="https://www.cc.gatech.edu/~jyang375/">Jianwei Yang</a>  <br><a href="https://www.cc.gatech.edu/~jlu347/">Jiasen Lu</a> <br><a href="http://web.engr.oregonstate.edu/~leestef/">Stefan Lee</a> <br><a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a> <br><a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a></p><p><a href="https://www.gatech.edu/">Georgia Institute of Technology</a></p></blockquote></li><li><p><a href="https://arxiv.org/abs/1903.03326"><strong><em>“Knowledge-embedded routing network for scene graph generation”</em></strong></a></p><blockquote><p>CVPR 2019<br><a href="http://tianshuichen.com/">Tianshui Chen</a> <br><a href="https://whyu.me/">Weihao Yu</a> <br><a href="https://deepai.org/profile/riquan-chen">Riquan Chen</a> <br><a href="http://www.linliang.net/">Liang Lin</a></p><p>Sun Yat-Sen University DarkMatter AI Research</p></blockquote></li></ul><ul><li><a href="https://www.semanticscholar.org/paper/Graphical-Contrastive-Losses-for-Scene-Graph-Zhang-Shih/debca9b7eccca1c1b704df0fbe187f56cd869842"><strong><em>“Graphical contrastive losses for scene graph parsing”</em></strong></a><blockquote><p>CVPR 2019</p><p><a href="https://www.researchgate.net/profile/Ji_Zhang63">Ji Zhang</a> <br><a href="https://www.kevinjshih.com/">Kevin J. Shih</a> <br><a href="https://sites.rutgers.edu/ahmed-elgammal/">Ahmed Elgammal</a> <br><a href="https://scholar.google.com/citations?user=Wel9l1wAAAAJ&hl=en">Andrew Tao</a> <br><a href="https://ctnzr.io/publications/">Bryan Catanzaro</a></p><p>Department of Computer Science, Rutgers University</p></blockquote></li></ul><ul><li><p><a href="https://arxiv.org/abs/1812.01880"><strong><em>“Learning to compose dynamic tree structures for visual contexts”</em></strong></a></p><blockquote><p>CVPR 2019</p><p><a href="https://kaihuatang.github.io/">Kaihua Tang</a> <br><a href="https://www.researchgate.net/scientific-contributions/Hanwang-Zhang-2144620148">Hanwang Zhang</a> <br><a href="https://sds.cuhk.edu.cn/teacher/322">Baoyuan Wu</a> <br><a href="https://whluo.github.io/">Wenhan Luo</a> <br><a href="https://sse.cuhk.edu.cn/en/faculty/liuwei">Wei Liu</a></p></blockquote></li><li><p><a href="https://arxiv.org/abs/1904.00560"><strong><em>“Scene graph generation with external knowledge and image reconstruction”</em></strong></a></p><blockquote><p>CVPR 2019</p><p><a href="https://gujiuxiang.com/">Jiuxiang Gu</a> <br><a href="https://hdzhao.github.io/">Handong Zhao</a> <br><a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a> <br><a href="http://cobweb.cs.uga.edu/~shengli/">Sheng Li</a> <br><a href="https://research.monash.edu/en/persons/jianfei-cai">Jianfei Cai</a> <br><a href="https://gujiuxiang.com/authors/mingyang-ling/">Mingyang Ling</a></p><p>ROSE Lab, Interdisciplinary Graduate School, Nanyang Technological University, Singapore</p></blockquote></li><li><p><a href="https://arxiv.org/abs/1904.11622"><strong><em>“Scene graph prediction with limited labels”</em></strong></a></p><blockquote><p>ICCV 2019</p><p><a href="https://vincentsc.com/">Vincent S. Chen</a> <br><a href="https://www.paroma.xyz/">Paroma Varma</a> <br><a href="http://www.ranjaykrishna.com/index.html">Ranjay Krishna</a> <br><a href="https://hci.stanford.edu/msb/">Michael Bernstein</a> <br><a href="https://cs.stanford.edu/~chrismre/">Christopher Re</a> <br><a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei </a></p><p>Stanford University</p></blockquote></li><li><p><a href="https://arxiv.org/abs/1812.02347"><strong><em>“Counterfactual critic multi-agent training for scene graph generation”</em></strong></a></p><blockquote><p>ICCV 2019 (oral)</p><p><a href="https://zjuchenlong.github.io/">Long Chen</a> <br><a href="https://research.ntu.edu.sg/expertise/academicprofile/Pages/StaffProfile.aspx?ST_EMAILID=hanwangzhang&CategoryDescription=Mathematics">Hanwang Zhang</a> \ (<a href="https://www.researchgate.net/scientific-contributions/Hanwang-Zhang-2144620148">https://www.researchgate.net/scientific-contributions/Hanwang-Zhang-2144620148</a>)  \ (<a href="https://dl.acm.org/profile/83358928557">https://dl.acm.org/profile/83358928557</a>) <br><a href="https://person.zju.edu.cn/junx#0">Jun Xiao</a> <br><a href="http://staff.ustc.edu.cn/~hexn/">Xiangnan He</a> <br><a href="https://dl.acm.org/profile/99658666631">Shiliang Pu</a> \ (<a href="https://www.researchgate.net/scientific-contributions/Shiliang-Pu-2116850663">https://www.researchgate.net/scientific-contributions/Shiliang-Pu-2116850663</a>) <br><a href="https://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a></p><p>DCD Lab, College of Computer Science and Technology, Zhejiang University</p></blockquote></li><li><p><a href="https://cs.stanford.edu/people/ranjaykrishna/graphfunctions/index.html"><strong><em>“Visual Relationships as Functions: Enabling Few-Shot Scene Graph Predictio”</em></strong></a></p></li><li><p><a href="https://arxiv.org/abs/1906.04876"><strong><em>“Learning Predicates as Functions to Enable Few-shot Scene Graph Prediction” arXiv检索</em></strong></a>  </p><blockquote><p>ICCV 2019 workshop</p><p><a href="https://dblp.org/pid/242/9136.html">Apoorva Dornadula</a> <br><a href="https://www.researchgate.net/scientific-contributions/Austin-Narcomey-2155538242">Austin Narcomey </a><br><a href="http://www.ranjaykrishna.com/index.html">Ranjay Krishna</a> <br><a href="https://hci.stanford.edu/msb/">Michael Bernstein</a> <br><a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei </a></p><p>Stanford University</p></blockquote></li><li><p><a href="https://arxiv.org/abs/2002.11949"><strong><em>“Unbiased Scene Graph Generation from Biased Training”</em></strong></a></p><blockquote><p>CVPR 2020</p><p><a href="https://kaihuatang.github.io/">Kaihua Tang</a> <br><a href="https://yuleiniu.github.io/">Yulei Niu</a> <br>Jianqiang Huang <br><a href="http://jiaxins.io/">Jiaxin Shi</a> <br><a href="https://research.ntu.edu.sg/expertise/academicprofile/Pages/StaffProfile.aspx?ST_EMAILID=hanwangzhang&CategoryDescription=Mathematics">Hanwang Zhang</a> \ (<a href="https://www.researchgate.net/scientific-contributions/Hanwang-Zhang-2144620148">https://www.researchgate.net/scientific-contributions/Hanwang-Zhang-2144620148</a>) \   (<a href="https://dl.acm.org/profile/83358928557">https://dl.acm.org/profile/83358928557</a>)</p><p>Nanyang Technological University</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SGG </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态融合——以KDD2020两篇论文为参考</title>
      <link href="2020/10/18/multimodal-kdd2020/"/>
      <url>2020/10/18/multimodal-kdd2020/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><h2 id="2-HGMF-Heterogeneous-Graph-based-Fusion-for-Multimodal-Data-with-Incompleteness"><a href="#2-HGMF-Heterogeneous-Graph-based-Fusion-for-Multimodal-Data-with-Incompleteness" class="headerlink" title="2 HGMF: Heterogeneous Graph-based Fusion for Multimodal Data with Incompleteness"></a>2 HGMF: Heterogeneous Graph-based Fusion for Multimodal Data with Incompleteness</h2><p>该论文提出了一种新的解决模态数据缺失的方法。</p><h2 id="3-Improving-Conversational-Recommender-Systems-via-Knowledge-Graph-based-Semantic-Fusion"><a href="#3-Improving-Conversational-Recommender-Systems-via-Knowledge-Graph-based-Semantic-Fusion" class="headerlink" title="3 Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion"></a>3 Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion</h2><h2 id="附录A：概念说明"><a href="#附录A：概念说明" class="headerlink" title="附录A：概念说明"></a>附录A：概念说明</h2><hr><p><code>sentiment analysis</code>: 情感分析</p><p><code>emotion recognition</code> ： 情绪识别</p><p><code>disease diagnosis</code>：疾病诊断</p><p><code>semi-supervised learning</code>  ： 半监督学习</p><blockquote><p>学习算法不需要人工干预，基于自身对未标记数据的利用提高模型的泛化能力</p><p>一般认为半监督学习分为纯监督学习（pure semi-supervised learning）和直推式学习（transductive learning）</p></blockquote><p><code>transductive learning framework</code>: 直推式学习框架</p><blockquote><p>直推学习是指通过观察特定的训练样本，进而预测特定测试样本的方法</p><p>直推学习和半监督学习一样，不需要人工干预，不同之处在于直推学习假设未标记的数据即最终用来测试的数据，其目的是在这些数据上具有最佳的泛化能力，而半监督学习则是学习时不知道最终的测试用例是什么，直推式学习可以看做半监督学习的一个特例</p><p>同时使用训练样本和测试样本训练模型，然后使用测试样本来测试模型效果</p></blockquote><p><code>inductive learning</code>： 归纳式学习</p><blockquote><p>从训练样本中学习规则然后应用于测试样本中，常用的监督学习就是归纳学习</p></blockquote><hr><p><code>early fusion</code>：在特征上（<code>future level</code>）上进行融合，进行不同特征的连接，输入到一个分类器模型中进行训练，融合发生在特征之间</p><p><code>late fusion</code>：也叫作<code>decision fusion</code>,指的是在预测分数（<code>score-level</code>）上进行融合，做法是训练多个模型每一个模型都会有相应的预测评分，对所有的模型结果进行<code>fusion</code>，得到最后的预测结果。在这类融合的过程中不同的特征使用不同的分类器模型进行特征分类</p><blockquote><p>相关论文：</p><ul><li>Select-additive learning: Improving generalization in multimodal sentiment analysis</li><li>Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</li></ul></blockquote><hr><p><code>graph-based late fusion</code>： 基于图的late fusion</p>]]></content>
      
      
      <categories>
          
          <category> 图处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multimodal </tag>
            
            <tag> KDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SGG领域工作梳理</title>
      <link href="2020/10/11/scene-graph-gerneration/"/>
      <url>2020/10/11/scene-graph-gerneration/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="场景图生成概述"><a href="#场景图生成概述" class="headerlink" title="场景图生成概述"></a>场景图生成概述</h2><p>场景图概念最先提出是在2015年stanford University的Justin Johnson等人提出的，目的是实现图像的检索任务（images retrieval using scene graph），通过将传统的文本转化为场景图之后作为输入传入检索模型，在模型中使用输入的文本语义图和场景图进行匹配，从而实现对于图像的更为精确的检索。</p><p>在Johnson的论文中定义的scene graph如下：</p><blockquote><p>A scene graph is a data structure that describes the contents of a scene. A scene graph encodes object instances, attributes of objects, and relationships between objects.</p></blockquote><p>简而言之，场景图就是一种描述图片中对象实体和属性以及实体间关系的一种数据结构。具体的结构定义如下所示：</p><p><img src="/images/SGG_define.PNG"></p><p>可以看到由于场景图将图片的对象及其属性信息，乃至对象之间的关系全都纳入到结构化的图数据结构中，使得计算机对于图像的语义信息表达更加准确和完善，因此场景图的出现极大的刺激了图像处理相关技术的发展。</p><p>之后，基于场景图的工作越来越多，由于最初的数据集中的场景图由人工进行标注，同时一些新的下游任务诸如图片生成，自动标注任务等等需要实现图片场景图的自动生成，因而越来越多的工作开始集中于场景图的自动生成，该领域也越来越火热。近几年在计算机视觉领域的顶会上，有关场景图的工作越来越多，同时随着场景图生成质量的提高，相关下游任务的工作也逐渐变多起来。</p><h2 id="应用方向"><a href="#应用方向" class="headerlink" title="应用方向"></a>应用方向</h2><h2 id="领域发展情况"><a href="#领域发展情况" class="headerlink" title="领域发展情况"></a>领域发展情况</h2><h2 id="常用方法总结"><a href="#常用方法总结" class="headerlink" title="常用方法总结"></a>常用方法总结</h2><h2 id="常用指标总结"><a href="#常用指标总结" class="headerlink" title="常用指标总结"></a>常用指标总结</h2><h2 id="一点点个人思考"><a href="#一点点个人思考" class="headerlink" title="一点点个人思考"></a>一点点个人思考</h2><h2 id="参考文献列表"><a href="#参考文献列表" class="headerlink" title="参考文献列表"></a>参考文献列表</h2><p>[1] Justin Johnson. Image Retrieval using Scene Graph. CVPR 2015</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SGG </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人博客搭建[hexo+butterfly+github pages]</title>
      <link href="2020/09/30/blog-building/"/>
      <url>2020/09/30/blog-building/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>想搭建博客很久了，github站都已经建了一年多了，趁着这个国庆假期搭建一下吧。</p><h2 id="建站准备"><a href="#建站准备" class="headerlink" title="建站准备"></a>建站准备</h2><ol><li><p>下载并安装<a href="https://www.git-scm.com/downloads">git</a>和<a href="http://nodejs.cn/download/">node</a></p><p> 安装git后相关使用方法可以看一下<a href="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰的git教程</a></p><p> nodejs安装之后需要在命令行中输入命令<code>npm -v</code> 和<code>node -v</code>,输出版本号则说明安装成功</p><p> <img src="/images/node_set.PNG"></p></li><li><p>初始化hexo环境</p><p> 输入命令<code>npm install -g hexo-cli</code> 安装hexo</p><p> 在本地文件夹下打开命令行，输入<code>hexo init &lt;blog_name&gt;</code>,初始化博客目录</p><p> 之后按一下的命令执行：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd &lt;blog_name&gt;</span><br><span class="line">npm install </span><br><span class="line">hexo clean </span><br><span class="line">hexo g      &#x2F;&#x2F;生成静态页面</span><br><span class="line">hexo s      &#x2F;&#x2F;在本地运行博客</span><br></pre></td></tr></table></figure><p> 然后浏览器访问<code>https://localhost:4000</code>即可访问初始页面，能够访问初始页面也说明安装成功了。</p><p> <img src="/images/hexo_page.PNG"></p></li></ol><h2 id="hexo博客美化"><a href="#hexo博客美化" class="headerlink" title="hexo博客美化"></a>hexo博客美化</h2><h3 id="主题选择"><a href="#主题选择" class="headerlink" title="主题选择"></a>主题选择</h3><p>目前的hexo的主题可选性很多，建议直接在github中搜索<code>hexo theme</code>，可以找到很多主题的仓库，在主页中可以很看一下<code>demo</code>，找到自己喜欢的主题，目前主流的主题<code>next</code>用的比较广泛，但是本站采用<a href="https://github.com/jerryc127/hexo-theme-butterfly">butterfly</a>这个主题.</p><p>主题选择之后在博客的根目录中通过<code>git clone -b master https://github.com/jerryc127/hexo -theme -butterfly .git themes/butterfly</code>命令下载到本机的<code>themes</code>目录下</p><blockquote><p>注意不要直接<code>git clone</code>到<code>themes</code>目录下，最好按照<a href="https://demo.jerryc.me/posts/21cfbf15/#%E5%AE%89%E8%A3%9D">官方文档</a>的说明进行配置。</p></blockquote><p>之后打开<code>hexo</code>根目录下的<code>_config.yml</code>文件进行站点配置。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      
      
      <categories>
          
          <category> 常用技术操作备忘录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> blog building </tag>
            
            <tag> butterfly </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/09/30/hello-world/"/>
      <url>2020/09/30/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
